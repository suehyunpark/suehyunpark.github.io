<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sue Hyun Park </title> <meta name="author" content="Sue Hyun Park"> <meta name="description" content="Sue Hyun Park is an M.S. student in KAIST AI. Her research interests include natural language processing, hallucination, and causal reasoning. "> <meta name="keywords" content="Sue Hyun Park, Sue Hyun, Park, KAIST, AI, Artificial Intelligence, Machine Learning, Deep Learning, Natural Language Processing, NLP, Hallucination, Causal Reasoning, 박수현"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?43ccc75b5ea68e84fb255ec8c4e5092e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://suehyunpark.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?7a860bb1099f1ce3f7e41588816ec311"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Sue Hyun Park </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?fde2a60705e466743b4221e4cd17ff0e" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%75%65%68%79%75%6E%70%61%72%6B@%6B%61%69%73%74.%61%63.%6B%72" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=AyZdps8AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/suehyunpark" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/sue-hyun-park-2682ba1a0" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/suehpark" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </div> <div class="clearfix"> <p>Hello! I am a first year M.S. student in <a href="https://gsai.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">KAIST Graduate School of AI (KAIST AI)</a>. I am fortunate to be advised by <a href="https://seominjoon.github.io/" rel="external nofollow noopener" target="_blank">Minjoon Seo</a> in <a href="https://LKLab.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">Language &amp; Knowledge Lab (LKLab)</a>. Previously, I received a B.B.A. and B.S. in Artificial Intelligence from <a href="https://en.snu.ac.kr/index.html" rel="external nofollow noopener" target="_blank">Seoul National University</a>. I have also interned at <a href="https://friendli.ai/" rel="external nofollow noopener" target="_blank">FriendliAI</a> in its early stage, envisioning the business impact of generative AI models.</p> <p>I am interested in how language models interpret and synthesize complex, context-rich information. I have analyzed <a href="https://arxiv.org/abs/2311.07362" rel="external nofollow noopener" target="_blank">hallucinative behavior</a> and <a href="https://arxiv.org/abs/2401.06591" rel="external nofollow noopener" target="_blank">fine-grained evaluation capabilities</a> of vision-language models and am currently working on evaluating the depth of model responses in interactions with human users. I am also exploring the principles of deliberative thinking and causal reasoning to improve how language models comprehend and articulate the complex interplay among concepts, values, and behaviors humans possess.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jan 12, 2024</th> <td> Our new preprint, <a href="https://arxiv.org/abs/2401.06591" rel="external nofollow noopener" target="_blank">Prometheus-Vision</a>, is out with <a href="https://github.com/kaistAI/prometheus-vision" rel="external nofollow noopener" target="_blank">code</a>, <a href="https://huggingface.co/collections/kaist-ai/the-perception-collection-65a4c60bea987387680e95c6" rel="external nofollow noopener" target="_blank">model checkpoints and datasets</a>! Check out our <a href="https://kaistai.github.io/prometheus-vision/" rel="external nofollow noopener" target="_blank">project page</a> too! </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 14, 2023</th> <td> Our new preprint, <a href="https://arxiv.org/abs/2311.07362" rel="external nofollow noopener" target="_blank">Volcano</a>, is out with <a href="https://huggingface.co/kaist-ai/volcano-7b" rel="external nofollow noopener" target="_blank">model</a> <a href="https://huggingface.co/kaist-ai/volcano-13b" rel="external nofollow noopener" target="_blank">checkpoints</a> and <a href="https://huggingface.co/datasets/kaist-ai/volcano-train" rel="external nofollow noopener" target="_blank">dataset</a>. Stay tuned for the <a href="https://github.com/kaistAI/Volcano" rel="external nofollow noopener" target="_blank">code</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 28, 2023</th> <td> Starting a M.S. at <a href="https://gsai.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">KAIST AI</a>. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div id="lee2023volcano" class="col-sm-10"> <div class="title">Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision</div> <div class="author"> Seongyun Lee , <em>Sue Hyun Park</em> , Yongrae Jo , Minjoon Seo </div> <div class="periodical"> <em>arXiv preprint</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.07362" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/kaistAI/Volcano" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Large multimodal models (LMMs) suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination might be due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through a qualitative analysis, we show that Volcano’s feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information, helping alleviate multimodal hallucination. We publicly release Volcano models of 7B and 13B sizes along with the data and code at this https URL.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="lee2024prometheusvision" class="col-sm-10"> <div class="title">Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</div> <div class="author"> Seongyun Lee* , Seungone Kim* , <em>Sue Hyun Park</em> , Geewook Kim , Minjoon Seo </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2401.06591" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/kaistAI/prometheus-vision" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://kaistai.github.io/prometheus-vision/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at this https URL</p> </div> </div> </div> </li> </ol> </div> <h2> <a href="/services/" style="color: inherit">services</a> </h2> <div class="services"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 30%; vertical-align: top;"> Secondary Reviewer </th> <td> ACL 2023, EMNLP Industry Track 2023 </td> </tr> </table> </div> </div> <h2> <a href="/education/" style="color: inherit">education</a> </h2> <div class="education"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 30%; vertical-align: top;"> Aug 2023 - Aug 2025 (<em>expected</em>) </th> <td> KAIST AI<br> M.S. in Artificial Intelligence, Advisor: <a href="https://seominjoon.github.io/" rel="external nofollow noopener" target="_blank">Minjoon Seo</a> </td> </tr> <tr> <th scope="row" style="width: 30%; vertical-align: top;"> Mar 2018 - Aug 2023 </th> <td> Seoul National University (SNU)<br> B.B.A. &amp; B.S. in Artificial Intelligence, <em>Cum Laude</em> </td> </tr> </table> </div> </div> <h2> <a href="/experience/" style="color: inherit">work experience</a> </h2> <div class="experience"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 30%; vertical-align: top;"> Dec 2022 - Aug 2023 </th> <td> <a href="https://lklab.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">LKLab</a>, KAIST AI<br> Research Intern, Host: <a href="https://seominjoon.github.io/" rel="external nofollow noopener" target="_blank">Minjoon Seo</a> </td> </tr> <tr> <th scope="row" style="width: 30%; vertical-align: top;"> Jul 2022 - Dec 2022 </th> <td> <a href="https://spl.snu.ac.kr/" rel="external nofollow noopener" target="_blank">SPLab</a>, Seoul National University<br> Research Intern, Host: <a href="https://bgchun.github.io/" rel="external nofollow noopener" target="_blank">Byung-gon Chun</a> </td> </tr> <tr> <th scope="row" style="width: 30%; vertical-align: top;"> Mar 2021 - Oct 2022 </th> <td> <a href="https://aiis.snu.ac.kr/eng/" rel="external nofollow noopener" target="_blank">Artificial Intelligence Institute of SNU</a><br> Technical Writer <a href="https://medium.com/snu-aiis-blog" rel="external nofollow noopener" target="_blank">[blog]</a> </td> </tr> <tr> <th scope="row" style="width: 30%; vertical-align: top;"> Feb 2021 - Dec 2021 </th> <td> <a href="https://friendli.ai/" rel="external nofollow noopener" target="_blank">FriendliAI</a><br> Corporate Development &amp; Technical Writing Intern </td> </tr> </table> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Sue Hyun Park. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?576a6f92371b70f065af650111742e4e"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?906085072389244b4db0c89c5b1dd28d" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>